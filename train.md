epoch: 0, time: 9.59/320.03, train_loss: 1.3289, val_loss: 0.2102, ER/F/LE/LR/SELD: 25.04/0.00/60.50/1.00/6.60, best_val_epoch: 0 (25.04/0.00/60.50/1.00/6.60)
epoch: 1, time: 9.64/42.39, train_loss: 0.0869, val_loss: 0.0194, ER/F/LE/LR/SELD: 1.13/0.00/105.77/0.07/0.91, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 2, time: 8.90/41.43, train_loss: 0.0370, val_loss: 0.0165, ER/F/LE/LR/SELD: 1.00/0.00/159.72/0.00/0.97, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 3, time: 8.56/41.21, train_loss: 0.0344, val_loss: 0.0144, ER/F/LE/LR/SELD: 1.00/0.00/171.59/0.00/0.99, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 4, time: 8.43/41.03, train_loss: 0.0327, val_loss: 0.0117, ER/F/LE/LR/SELD: 1.00/0.00/180.00/0.00/1.00, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 5, time: 8.21/41.21, train_loss: 0.0329, val_loss: 0.0139, ER/F/LE/LR/SELD: 1.00/0.00/170.35/0.00/0.99, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 6, time: 8.09/52.31, train_loss: 0.0346, val_loss: 0.0212, ER/F/LE/LR/SELD: 0.95/0.01/161.88/0.06/0.95, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 7, time: 8.19/41.34, train_loss: 0.0369, val_loss: 0.0136, ER/F/LE/LR/SELD: 1.00/0.00/172.06/0.00/0.99, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 8, time: 7.99/41.17, train_loss: 0.0324, val_loss: 0.0150, ER/F/LE/LR/SELD: 1.00/0.00/180.00/0.00/1.00, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 9, time: 8.08/41.25, train_loss: 0.0318, val_loss: 0.0138, ER/F/LE/LR/SELD: 0.99/0.00/162.94/0.00/0.97, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 10, time: 8.22/51.25, train_loss: 0.0331, val_loss: 0.0185, ER/F/LE/LR/SELD: 1.05/0.00/172.42/0.04/0.99, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 11, time: 8.15/41.04, train_loss: 0.0338, val_loss: 0.0110, ER/F/LE/LR/SELD: 1.00/0.00/180.00/0.00/1.00, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 12, time: 8.09/41.49, train_loss: 0.0332, val_loss: 0.0161, ER/F/LE/LR/SELD: 0.97/0.00/172.21/0.02/0.98, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 13, time: 7.92/41.13, train_loss: 0.0331, val_loss: 0.0127, ER/F/LE/LR/SELD: 1.00/0.00/171.38/0.00/0.99, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 14, time: 8.00/40.94, train_loss: 0.0330, val_loss: 0.0133, ER/F/LE/LR/SELD: 0.99/0.00/165.14/0.01/0.97, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 15, time: 8.07/41.02, train_loss: 0.0356, val_loss: 0.0143, ER/F/LE/LR/SELD: 1.00/0.00/180.00/0.00/1.00, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 16, time: 8.20/40.93, train_loss: 0.0334, val_loss: 0.0132, ER/F/LE/LR/SELD: 1.00/0.00/180.00/0.00/1.00, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 17, time: 8.08/41.01, train_loss: 0.0334, val_loss: 0.0140, ER/F/LE/LR/SELD: 1.00/0.00/170.89/0.00/0.99, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 18, time: 8.50/41.27, train_loss: 0.0320, val_loss: 0.0133, ER/F/LE/LR/SELD: 1.00/0.00/180.00/0.00/1.00, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 19, time: 8.17/40.93, train_loss: 0.0313, val_loss: 0.0122, ER/F/LE/LR/SELD: 1.00/0.00/180.00/0.00/1.00, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 20, time: 8.04/41.32, train_loss: 0.0328, val_loss: 0.0122, ER/F/LE/LR/SELD: 1.00/0.00/170.75/0.00/0.99, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 21, time: 8.06/41.39, train_loss: 0.0319, val_loss: 0.0136, ER/F/LE/LR/SELD: 1.00/0.00/180.00/0.00/1.00, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 22, time: 8.14/41.20, train_loss: 0.0314, val_loss: 0.0129, ER/F/LE/LR/SELD: 1.00/0.00/180.00/0.00/1.00, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 23, time: 8.16/43.65, train_loss: 0.0332, val_loss: 0.0152, ER/F/LE/LR/SELD: 0.98/0.01/163.30/0.02/0.97, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 24, time: 8.25/40.99, train_loss: 0.0317, val_loss: 0.0107, ER/F/LE/LR/SELD: 1.00/0.00/180.00/0.00/1.00, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 25, time: 7.99/40.90, train_loss: 0.0308, val_loss: 0.0117, ER/F/LE/LR/SELD: 1.00/0.00/180.00/0.00/1.00, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 26, time: 8.11/41.06, train_loss: 0.0307, val_loss: 0.0118, ER/F/LE/LR/SELD: 1.00/0.00/180.00/0.00/1.00, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 27, time: 8.02/41.21, train_loss: 0.0314, val_loss: 0.0126, ER/F/LE/LR/SELD: 1.00/0.00/173.41/0.00/0.99, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 28, time: 8.10/40.93, train_loss: 0.0305, val_loss: 0.0129, ER/F/LE/LR/SELD: 1.00/0.00/171.27/0.00/0.99, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 29, time: 7.92/41.00, train_loss: 0.0317, val_loss: 0.0132, ER/F/LE/LR/SELD: 1.00/0.00/171.50/0.00/0.99, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 30, time: 8.23/40.79, train_loss: 0.0301, val_loss: 0.0114, ER/F/LE/LR/SELD: 1.00/0.00/180.00/0.00/1.00, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 31, time: 8.37/40.97, train_loss: 0.0302, val_loss: 0.0110, ER/F/LE/LR/SELD: 1.00/0.00/180.00/0.00/1.00, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 32, time: 8.41/40.85, train_loss: 0.0299, val_loss: 0.0110, ER/F/LE/LR/SELD: 1.00/0.00/180.00/0.00/1.00, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 33, time: 8.34/41.02, train_loss: 0.0300, val_loss: 0.0135, ER/F/LE/LR/SELD: 1.00/0.00/180.00/0.00/1.00, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 34, time: 8.42/40.87, train_loss: 0.0306, val_loss: 0.0116, ER/F/LE/LR/SELD: 1.00/0.00/171.98/0.00/0.99, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 35, time: 8.23/41.28, train_loss: 0.0305, val_loss: 0.0135, ER/F/LE/LR/SELD: 1.00/0.00/171.66/0.00/0.99, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 36, time: 7.98/40.81, train_loss: 0.0293, val_loss: 0.0100, ER/F/LE/LR/SELD: 1.00/0.00/172.19/0.00/0.99, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 37, time: 8.09/41.43, train_loss: 0.0294, val_loss: 0.0108, ER/F/LE/LR/SELD: 1.00/0.00/170.72/0.01/0.99, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 38, time: 8.30/41.06, train_loss: 0.0296, val_loss: 0.0126, ER/F/LE/LR/SELD: 1.01/0.00/162.55/0.02/0.97, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 39, time: 8.67/41.38, train_loss: 0.0282, val_loss: 0.0108, ER/F/LE/LR/SELD: 0.99/0.01/169.03/0.01/0.98, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 40, time: 8.62/41.02, train_loss: 0.0279, val_loss: 0.0131, ER/F/LE/LR/SELD: 1.00/0.00/170.59/0.01/0.99, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 41, time: 8.43/40.79, train_loss: 0.0283, val_loss: 0.0120, ER/F/LE/LR/SELD: 1.00/0.00/157.96/0.01/0.97, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 42, time: 8.13/41.21, train_loss: 0.0285, val_loss: 0.0119, ER/F/LE/LR/SELD: 1.00/0.00/143.40/0.02/0.94, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 43, time: 7.91/40.91, train_loss: 0.0281, val_loss: 0.0116, ER/F/LE/LR/SELD: 1.00/0.00/147.58/0.02/0.95, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 44, time: 8.08/41.64, train_loss: 0.0278, val_loss: 0.0119, ER/F/LE/LR/SELD: 1.01/0.00/141.74/0.04/0.94, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 45, time: 8.17/41.26, train_loss: 0.0270, val_loss: 0.0100, ER/F/LE/LR/SELD: 1.00/0.00/143.63/0.02/0.94, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 46, time: 8.35/41.50, train_loss: 0.0270, val_loss: 0.0133, ER/F/LE/LR/SELD: 0.99/0.00/136.50/0.03/0.93, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 47, time: 8.43/41.02, train_loss: 0.0282, val_loss: 0.0135, ER/F/LE/LR/SELD: 1.00/0.00/139.02/0.02/0.94, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 48, time: 8.33/41.08, train_loss: 0.0271, val_loss: 0.0107, ER/F/LE/LR/SELD: 1.00/0.00/153.61/0.01/0.96, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 49, time: 8.17/41.51, train_loss: 0.0264, val_loss: 0.0114, ER/F/LE/LR/SELD: 1.02/0.00/145.23/0.03/0.95, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 50, time: 8.03/41.12, train_loss: 0.0266, val_loss: 0.0119, ER/F/LE/LR/SELD: 1.00/0.00/143.26/0.01/0.95, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 51, time: 7.93/41.33, train_loss: 0.0260, val_loss: 0.0146, ER/F/LE/LR/SELD: 1.01/0.00/141.99/0.01/0.95, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 52, time: 8.06/44.21, train_loss: 0.0276, val_loss: 0.0164, ER/F/LE/LR/SELD: 1.00/0.00/138.82/0.01/0.94, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 53, time: 8.29/41.29, train_loss: 0.0267, val_loss: 0.0119, ER/F/LE/LR/SELD: 1.01/0.00/144.99/0.03/0.95, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 54, time: 8.09/41.25, train_loss: 0.0264, val_loss: 0.0133, ER/F/LE/LR/SELD: 1.00/0.00/153.99/0.01/0.96, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 55, time: 8.08/41.20, train_loss: 0.0250, val_loss: 0.0143, ER/F/LE/LR/SELD: 0.99/0.01/137.86/0.01/0.93, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 56, time: 7.95/41.20, train_loss: 0.0265, val_loss: 0.0135, ER/F/LE/LR/SELD: 0.99/0.01/134.11/0.01/0.93, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 57, time: 7.99/41.15, train_loss: 0.0264, val_loss: 0.0134, ER/F/LE/LR/SELD: 1.00/0.01/147.70/0.02/0.95, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 58, time: 7.93/41.37, train_loss: 0.0253, val_loss: 0.0133, ER/F/LE/LR/SELD: 1.01/0.00/143.02/0.03/0.94, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 59, time: 8.10/41.69, train_loss: 0.0249, val_loss: 0.0144, ER/F/LE/LR/SELD: 0.99/0.01/137.61/0.03/0.93, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 60, time: 8.34/41.58, train_loss: 0.0250, val_loss: 0.0131, ER/F/LE/LR/SELD: 0.99/0.00/136.55/0.04/0.93, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 61, time: 8.16/41.62, train_loss: 0.0251, val_loss: 0.0120, ER/F/LE/LR/SELD: 1.01/0.01/141.64/0.02/0.94, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 62, time: 8.08/41.45, train_loss: 0.0246, val_loss: 0.0127, ER/F/LE/LR/SELD: 1.01/0.01/136.11/0.03/0.93, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 63, time: 8.10/41.96, train_loss: 0.0244, val_loss: 0.0120, ER/F/LE/LR/SELD: 1.00/0.00/138.08/0.03/0.93, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 64, time: 7.97/41.65, train_loss: 0.0246, val_loss: 0.0131, ER/F/LE/LR/SELD: 0.97/0.00/136.34/0.04/0.92, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 65, time: 7.99/41.83, train_loss: 0.0240, val_loss: 0.0152, ER/F/LE/LR/SELD: 1.04/0.01/143.22/0.04/0.94, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 66, time: 8.05/41.66, train_loss: 0.0236, val_loss: 0.0139, ER/F/LE/LR/SELD: 0.99/0.02/135.36/0.05/0.92, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 67, time: 8.08/42.09, train_loss: 0.0231, val_loss: 0.0126, ER/F/LE/LR/SELD: 1.03/0.01/138.32/0.04/0.94, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 68, time: 8.30/41.53, train_loss: 0.0239, val_loss: 0.0142, ER/F/LE/LR/SELD: 0.99/0.01/134.75/0.04/0.92, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 69, time: 8.21/57.96, train_loss: 0.0235, val_loss: 0.0238, ER/F/LE/LR/SELD: 1.00/0.02/137.35/0.08/0.92, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 70, time: 8.35/41.95, train_loss: 0.0242, val_loss: 0.0126, ER/F/LE/LR/SELD: 1.01/0.01/140.24/0.05/0.93, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 71, time: 8.11/42.79, train_loss: 0.0228, val_loss: 0.0150, ER/F/LE/LR/SELD: 1.03/0.02/134.96/0.05/0.93, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 72, time: 8.04/47.03, train_loss: 0.0228, val_loss: 0.0158, ER/F/LE/LR/SELD: 1.00/0.02/135.69/0.04/0.92, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 73, time: 8.08/43.14, train_loss: 0.0225, val_loss: 0.0175, ER/F/LE/LR/SELD: 1.04/0.03/134.94/0.04/0.93, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 74, time: 8.44/41.67, train_loss: 0.0230, val_loss: 0.0133, ER/F/LE/LR/SELD: 0.99/0.01/138.52/0.04/0.93, best_val_epoch: 1 (1.13/0.00/105.77/0.07/0.91)
epoch: 75, time: 8.46/41.95, train_loss: 0.0232, val_loss: 0.0127, ER/F/LE/LR/SELD: 0.97/0.03/134.01/0.05/0.91, best_val_epoch: 75 (0.97/0.03/134.01/0.05/0.91)
epoch: 76, time: 8.08/41.89, train_loss: 0.0221, val_loss: 0.0136, ER/F/LE/LR/SELD: 0.99/0.03/134.53/0.04/0.92, best_val_epoch: 75 (0.97/0.03/134.01/0.05/0.91)
epoch: 77, time: 8.21/42.06, train_loss: 0.0221, val_loss: 0.0136, ER/F/LE/LR/SELD: 0.98/0.02/134.10/0.04/0.92, best_val_epoch: 75 (0.97/0.03/134.01/0.05/0.91)
epoch: 78, time: 8.09/41.70, train_loss: 0.0213, val_loss: 0.0136, ER/F/LE/LR/SELD: 0.99/0.03/134.13/0.04/0.91, best_val_epoch: 75 (0.97/0.03/134.01/0.05/0.91)
epoch: 79, time: 8.11/41.87, train_loss: 0.0217, val_loss: 0.0122, ER/F/LE/LR/SELD: 0.98/0.03/135.59/0.05/0.91, best_val_epoch: 75 (0.97/0.03/134.01/0.05/0.91)
epoch: 80, time: 8.12/41.94, train_loss: 0.0212, val_loss: 0.0153, ER/F/LE/LR/SELD: 1.03/0.02/135.02/0.05/0.93, best_val_epoch: 75 (0.97/0.03/134.01/0.05/0.91)
epoch: 81, time: 8.42/42.71, train_loss: 0.0205, val_loss: 0.0136, ER/F/LE/LR/SELD: 1.02/0.03/136.24/0.07/0.92, best_val_epoch: 75 (0.97/0.03/134.01/0.05/0.91)
epoch: 82, time: 8.38/41.56, train_loss: 0.0212, val_loss: 0.0116, ER/F/LE/LR/SELD: 0.99/0.02/136.73/0.04/0.92, best_val_epoch: 75 (0.97/0.03/134.01/0.05/0.91)
epoch: 83, time: 8.13/42.19, train_loss: 0.0205, val_loss: 0.0127, ER/F/LE/LR/SELD: 1.01/0.03/134.51/0.06/0.92, best_val_epoch: 75 (0.97/0.03/134.01/0.05/0.91)
epoch: 84, time: 8.22/41.95, train_loss: 0.0211, val_loss: 0.0117, ER/F/LE/LR/SELD: 0.99/0.03/134.30/0.05/0.91, best_val_epoch: 75 (0.97/0.03/134.01/0.05/0.91)
epoch: 85, time: 8.06/42.11, train_loss: 0.0201, val_loss: 0.0132, ER/F/LE/LR/SELD: 0.98/0.03/134.01/0.05/0.91, best_val_epoch: 75 (0.97/0.03/134.01/0.05/0.91)
epoch: 86, time: 8.09/42.26, train_loss: 0.0200, val_loss: 0.0127, ER/F/LE/LR/SELD: 1.00/0.03/134.65/0.06/0.91, best_val_epoch: 75 (0.97/0.03/134.01/0.05/0.91)
epoch: 87, time: 8.08/42.28, train_loss: 0.0195, val_loss: 0.0124, ER/F/LE/LR/SELD: 1.02/0.03/125.52/0.06/0.91, best_val_epoch: 87 (1.02/0.03/125.52/0.06/0.91)
epoch: 88, time: 8.24/42.92, train_loss: 0.0195, val_loss: 0.0131, ER/F/LE/LR/SELD: 1.02/0.03/126.58/0.07/0.91, best_val_epoch: 88 (1.02/0.03/126.58/0.07/0.91)
epoch: 89, time: 8.16/42.71, train_loss: 0.0194, val_loss: 0.0115, ER/F/LE/LR/SELD: 1.02/0.03/128.62/0.08/0.91, best_val_epoch: 88 (1.02/0.03/126.58/0.07/0.91)
epoch: 90, time: 8.31/42.89, train_loss: 0.0188, val_loss: 0.0128, ER/F/LE/LR/SELD: 1.02/0.04/134.86/0.07/0.92, best_val_epoch: 88 (1.02/0.03/126.58/0.07/0.91)
epoch: 91, time: 8.13/43.16, train_loss: 0.0189, val_loss: 0.0153, ER/F/LE/LR/SELD: 1.07/0.03/127.76/0.08/0.92, best_val_epoch: 88 (1.02/0.03/126.58/0.07/0.91)
epoch: 92, time: 8.09/42.28, train_loss: 0.0186, val_loss: 0.0128, ER/F/LE/LR/SELD: 0.99/0.04/134.48/0.06/0.91, best_val_epoch: 88 (1.02/0.03/126.58/0.07/0.91)
epoch: 93, time: 8.05/42.91, train_loss: 0.0183, val_loss: 0.0133, ER/F/LE/LR/SELD: 1.05/0.04/127.37/0.09/0.91, best_val_epoch: 88 (1.02/0.03/126.58/0.07/0.91)
epoch: 94, time: 8.13/42.22, train_loss: 0.0179, val_loss: 0.0142, ER/F/LE/LR/SELD: 1.02/0.03/128.48/0.06/0.91, best_val_epoch: 88 (1.02/0.03/126.58/0.07/0.91)
epoch: 95, time: 8.40/43.26, train_loss: 0.0182, val_loss: 0.0120, ER/F/LE/LR/SELD: 1.14/0.03/125.48/0.08/0.93, best_val_epoch: 88 (1.02/0.03/126.58/0.07/0.91)
epoch: 96, time: 8.28/42.47, train_loss: 0.0175, val_loss: 0.0131, ER/F/LE/LR/SELD: 1.02/0.02/138.69/0.07/0.93, best_val_epoch: 88 (1.02/0.03/126.58/0.07/0.91)
epoch: 97, time: 8.50/43.06, train_loss: 0.0169, val_loss: 0.0126, ER/F/LE/LR/SELD: 1.04/0.04/126.23/0.08/0.91, best_val_epoch: 97 (1.04/0.04/126.23/0.08/0.91)
epoch: 98, time: 8.33/43.10, train_loss: 0.0161, val_loss: 0.0123, ER/F/LE/LR/SELD: 1.03/0.04/126.43/0.07/0.91, best_val_epoch: 97 (1.04/0.04/126.23/0.08/0.91)
epoch: 99, time: 8.23/42.92, train_loss: 0.0164, val_loss: 0.0109, ER/F/LE/LR/SELD: 1.03/0.03/127.65/0.08/0.91, best_val_epoch: 97 (1.04/0.04/126.23/0.08/0.91)
Load best model weights
Loading unseen test dataset:
Computing some stats about the dataset
        WARNING: Resetting batch size to 114. To accommodate the inference of longest file of 28461 frames in a single batch
        Datagen_mode: dev, nb_files: 78, nb_classes:13
        nb_frames_file: 28461, feat_len: 64, nb_ch: 7, label_len:None

        Dataset: foa, split: [4]
        batch_size: 114, feat_seq_len: 250, label_seq_len: 50, shuffle: False
        Total batches in dataset: 78
        label_dir: data2023/seld_feat_label/foa_dev_adpit_label
        feat_dir: data2023/seld_feat_label/foa_dev_norm

Dumping recording-wise test results in: results/3_1_dev_split0_multiaccdoa_foa_20240323021100_test

Test Loss
SELD score (early stopping metric): 0.90 [0.86, 0.92]
SED metrics: Error rate: 1.03 [0.98, 1.10], F-score: 4.3 [2.52, 5.90]
DOA metrics: Localization error: 125.5 [98.70 , 135.16], Localization Recall: 7.4 [5.49, 10.36]
Classwise results on unseen test data
Class   ER      F       LE      LR      SELD_score
0       1.03 [0.98, 1.10]       0.11 [0.05, 0.16]       36.74 [26.95, 45.79]    0.23 [0.14, 0.32]       0.72 [0.68, 0.77]
1       1.03 [0.98, 1.10]       0.06 [0.01, 0.10]       39.73 [30.29, 51.20]    0.14 [0.08, 0.21]       0.76 [0.73, 0.80]
2       1.03 [0.98, 1.10]       0.00 [0.00, 0.00]       180.00 [180.00, 180.00] 0.00 [0.00, 0.00]       1.01 [0.99, 1.03]
3       1.03 [0.98, 1.10]       0.00 [0.00, 0.00]       180.00 [180.00, 180.00] 0.00 [0.00, 0.00]       1.01 [0.99, 1.03]
4       1.03 [0.98, 1.10]       0.00 [0.00, 0.00]       180.00 [180.00, 180.00] 0.00 [0.00, 0.00]       1.01 [0.99, 1.03]
5       1.03 [0.98, 1.10]       0.27 [0.11, 0.40]       19.61 [10.25, 47.24]    0.34 [0.18, 0.60]       0.63 [0.55, 0.72]
6       1.03 [0.98, 1.10]       0.00 [0.00, 0.00]       180.00 [180.00, 180.00] 0.00 [0.00, 0.00]       1.01 [0.99, 1.03]
7       1.03 [0.98, 1.10]       0.00 [0.00, 0.00]       180.00 [180.00, 180.00] 0.00 [0.00, 0.00]       1.01 [0.99, 1.03]
8       1.03 [0.98, 1.10]       0.12 [-0.05, 0.31]      32.99 [2.71, 54.45]     0.25 [0.08, 0.44]       0.71 [0.58, 0.83]
9       1.03 [0.98, 1.10]       0.00 [0.00, 0.00]       62.08 [-286.13, 177.47] 0.00 [-0.00, 0.01]      0.84 [0.36, 1.00]
10      1.03 [0.98, 1.10]       0.00 [0.00, 0.00]       180.00 [180.00, 180.00] 0.00 [0.00, 0.00]       1.01 [0.99, 1.03]
11      1.03 [0.98, 1.10]       0.00 [0.00, 0.00]       180.00 [180.00, 180.00] 0.00 [0.00, 0.00]       1.01 [0.99, 1.03]
12      1.03 [0.98, 1.10]       0.00 [0.00, 0.00]       180.00 [180.00, 180.00] 0.00 [0.00, 0.00]       1.01 [0.99, 1.03]